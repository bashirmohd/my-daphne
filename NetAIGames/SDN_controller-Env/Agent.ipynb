{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import PPO2\n",
    "from SDNController import SDNController\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 1000\n",
    "max_ticks = 100\n",
    "rewards_check_freq = 100\n",
    "test_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                    print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SDNController(max_ticks = max_ticks)\n",
    "env = Monitor(env, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING:tensorflow:From /Users/bashirm/opt/anaconda3/lib/python3.7/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/bashirm/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/bashirm/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/bashirm/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Num timesteps: 100\n",
      "Best mean reward: -inf - Last mean reward per episode: -579.00\n",
      "Saving new best model to tmp/best_model\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0005469609  |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 100           |\n",
      "| ep_reward_mean     | -579          |\n",
      "| explained_variance | -0.000144     |\n",
      "| fps                | 268           |\n",
      "| n_updates          | 1             |\n",
      "| policy_entropy     | 1.0980407     |\n",
      "| policy_loss        | -0.0024280483 |\n",
      "| serial_timesteps   | 128           |\n",
      "| time_elapsed       | 1.81e-05      |\n",
      "| total_timesteps    | 128           |\n",
      "| value_loss         | 2964.6125     |\n",
      "--------------------------------------\n",
      "Num timesteps: 200\n",
      "Best mean reward: -579.00 - Last mean reward per episode: -527.50\n",
      "Saving new best model to tmp/best_model\n",
      "-------------------------------------\n",
      "| approxkl           | 0.000221937  |\n",
      "| clipfrac           | 0.0          |\n",
      "| ep_len_mean        | 100          |\n",
      "| ep_reward_mean     | -528         |\n",
      "| explained_variance | 0.000927     |\n",
      "| fps                | 1212         |\n",
      "| n_updates          | 2            |\n",
      "| policy_entropy     | 1.0949898    |\n",
      "| policy_loss        | 0.0012085658 |\n",
      "| serial_timesteps   | 256          |\n",
      "| time_elapsed       | 0.477        |\n",
      "| total_timesteps    | 256          |\n",
      "| value_loss         | 2267.12      |\n",
      "-------------------------------------\n",
      "Num timesteps: 300\n",
      "Best mean reward: -527.50 - Last mean reward per episode: -524.00\n",
      "Saving new best model to tmp/best_model\n",
      "--------------------------------------\n",
      "| approxkl           | 1.8557124e-05 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 100           |\n",
      "| ep_reward_mean     | -524          |\n",
      "| explained_variance | 0.000277      |\n",
      "| fps                | 1149          |\n",
      "| n_updates          | 3             |\n",
      "| policy_entropy     | 1.0947721     |\n",
      "| policy_loss        | 0.0004929594  |\n",
      "| serial_timesteps   | 384           |\n",
      "| time_elapsed       | 0.584         |\n",
      "| total_timesteps    | 384           |\n",
      "| value_loss         | 3501.377      |\n",
      "--------------------------------------\n",
      "Num timesteps: 400\n",
      "Best mean reward: -524.00 - Last mean reward per episode: -546.25\n",
      "Num timesteps: 500\n",
      "Best mean reward: -524.00 - Last mean reward per episode: -556.00\n",
      "---------------------------------------\n",
      "| approxkl           | 0.00010976305  |\n",
      "| clipfrac           | 0.0            |\n",
      "| ep_len_mean        | 100            |\n",
      "| ep_reward_mean     | -556           |\n",
      "| explained_variance | 0.000878       |\n",
      "| fps                | 1188           |\n",
      "| n_updates          | 4              |\n",
      "| policy_entropy     | 1.0938302      |\n",
      "| policy_loss        | -0.00066730427 |\n",
      "| serial_timesteps   | 512            |\n",
      "| time_elapsed       | 0.696          |\n",
      "| total_timesteps    | 512            |\n",
      "| value_loss         | 3160.4807      |\n",
      "---------------------------------------\n",
      "Num timesteps: 600\n",
      "Best mean reward: -524.00 - Last mean reward per episode: -552.17\n",
      "-------------------------------------\n",
      "| approxkl           | 5.412701e-05 |\n",
      "| clipfrac           | 0.0          |\n",
      "| ep_len_mean        | 100          |\n",
      "| ep_reward_mean     | -552         |\n",
      "| explained_variance | -0.000158    |\n",
      "| fps                | 1386         |\n",
      "| n_updates          | 5            |\n",
      "| policy_entropy     | 1.0938035    |\n",
      "| policy_loss        | 0.0011606376 |\n",
      "| serial_timesteps   | 640          |\n",
      "| time_elapsed       | 0.805        |\n",
      "| total_timesteps    | 640          |\n",
      "| value_loss         | 1944.0035    |\n",
      "-------------------------------------\n",
      "Num timesteps: 700\n",
      "Best mean reward: -524.00 - Last mean reward per episode: -557.29\n",
      "--------------------------------------\n",
      "| approxkl           | 3.1486816e-05 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 100           |\n",
      "| ep_reward_mean     | -557          |\n",
      "| explained_variance | 0.000143      |\n",
      "| fps                | 1118          |\n",
      "| n_updates          | 6             |\n",
      "| policy_entropy     | 1.0937043     |\n",
      "| policy_loss        | 0.00016544492 |\n",
      "| serial_timesteps   | 768           |\n",
      "| time_elapsed       | 0.898         |\n",
      "| total_timesteps    | 768           |\n",
      "| value_loss         | 4452.045      |\n",
      "--------------------------------------\n",
      "Num timesteps: 800\n",
      "Best mean reward: -524.00 - Last mean reward per episode: -583.75\n",
      "--------------------------------------\n",
      "| approxkl           | 6.3819294e-05 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 100           |\n",
      "| ep_reward_mean     | -584          |\n",
      "| explained_variance | -0.000272     |\n",
      "| fps                | 1383          |\n",
      "| n_updates          | 7             |\n",
      "| policy_entropy     | 1.0914944     |\n",
      "| policy_loss        | 0.0022675344  |\n",
      "| serial_timesteps   | 896           |\n",
      "| time_elapsed       | 1.01          |\n",
      "| total_timesteps    | 896           |\n",
      "| value_loss         | 3724.2559     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x1a4143cb90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "\n",
    "# Create the callback: check every 100 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=rewards_check_freq, log_dir=log_dir)\n",
    "\n",
    "model.learn(total_timesteps=time_steps, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_plotter.plot_results([log_dir], time_steps, results_plotter.X_TIMESTEPS, \"PPO SDNController\")\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "Current state:  [ 7.80282547  7.69531741  5.          6.12783093  6.40206957  2.\n",
      " 24.17223119 24.38579482 10.        ] \n",
      " Action:  0 \n",
      " Reward:  -15 \n",
      " Done:  False \n",
      " Next state:  [10.53608098  9.92999964  5.          7.80660019  7.74465674  2.\n",
      " 25.19832711 24.3295875  10.        ] \n",
      " \n",
      "\n",
      "Current state:  [10.53608098  9.92999964  5.          7.80660019  7.74465674  2.\n",
      " 25.19832711 24.3295875  10.        ] \n",
      " Action:  0 \n",
      " Reward:  -15 \n",
      " Done:  False \n",
      " Next state:  [11.79494215 12.36795913  5.          5.78629542  6.04074721  2.\n",
      " 24.34477586 23.81262865 10.        ] \n",
      " \n",
      "\n",
      "Current state:  [11.79494215 12.36795913  5.          5.78629542  6.04074721  2.\n",
      " 24.34477586 23.81262865 10.        ] \n",
      " Action:  2 \n",
      " Reward:  -20 \n",
      " Done:  False \n",
      " Next state:  [10.71054885 10.75153617  5.          7.55800336  7.17863276  2.\n",
      " 23.89548806 24.34090259 10.        ] \n",
      " \n",
      "\n",
      "Current state:  [10.71054885 10.75153617  5.          7.55800336  7.17863276  2.\n",
      " 23.89548806 24.34090259 10.        ] \n",
      " Action:  0 \n",
      " Reward:  5 \n",
      " Done:  False \n",
      " Next state:  [12.00467371 12.42284213  5.          4.78346209  4.48807077  2.\n",
      " 23.81435167 24.30223771 10.        ] \n",
      " \n",
      "\n",
      "Current state:  [12.00467371 12.42284213  5.          4.78346209  4.48807077  2.\n",
      " 23.81435167 24.30223771 10.        ] \n",
      " Action:  1 \n",
      " Reward:  -12 \n",
      " Done:  False \n",
      " Next state:  [11.89342063 11.89095849  5.          5.14737614  5.54156537  2.\n",
      " 23.28417638 23.11197659 10.        ] \n",
      " \n",
      "\n",
      "Current state:  [11.89342063 11.89095849  5.          5.14737614  5.54156537  2.\n",
      " 23.28417638 23.11197659 10.        ] \n",
      " Action:  0 \n",
      " Reward:  -15 \n",
      " Done:  False \n",
      " Next state:  [11.09495538 12.12429886  5.          3.4131393   3.42836793  2.\n",
      " 24.72873233 24.22999528 10.        ] \n",
      " \n",
      "\n",
      "Current state:  [11.09495538 12.12429886  5.          3.4131393   3.42836793  2.\n",
      " 24.72873233 24.22999528 10.        ] \n",
      " Action:  1 \n",
      " Reward:  8 \n",
      " Done:  False \n",
      " Next state:  [ 6.8688198   6.79163788  5.          6.2195084   5.65067475  2.\n",
      " 23.83573162 23.91441175 10.        ] \n",
      " \n",
      "\n",
      "Current state:  [ 6.8688198   6.79163788  5.          6.2195084   5.65067475  2.\n",
      " 23.83573162 23.91441175 10.        ] \n",
      " Action:  1 \n",
      " Reward:  -12 \n",
      " Done:  False \n",
      " Next state:  [11.49040105 12.20907683  5.          5.85616308  5.95633438  2.\n",
      " 24.60601198 24.25041464 10.        ] \n",
      " \n",
      "\n",
      "Current state:  [11.49040105 12.20907683  5.          5.85616308  5.95633438  2.\n",
      " 24.60601198 24.25041464 10.        ] \n",
      " Action:  0 \n",
      " Reward:  5 \n",
      " Done:  False \n",
      " Next state:  [ 8.66792137  9.23541908  5.          3.84470808  4.09436703  2.\n",
      " 22.73205131 23.47283464 10.        ] \n",
      " \n",
      "\n",
      "Current state:  [ 8.66792137  9.23541908  5.          3.84470808  4.09436703  2.\n",
      " 22.73205131 23.47283464 10.        ] \n",
      " Action:  2 \n",
      " Reward:  0 \n",
      " Done:  False \n",
      " Next state:  [ 6.57622375  7.20781792  5.          7.09458529  6.24401943  2.\n",
      " 24.48485349 25.09042957 10.        ] \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model = PPO2.load(\"./tmp/best_model.zip\")\n",
    "\n",
    "# Enjoy trained agent\n",
    "dones = True\n",
    "for ix in range(test_steps):\n",
    "    if dones:\n",
    "        obs = env.reset() \n",
    "        curr_state = obs\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    print('Current state: ', curr_state, '\\n', 'Action: ', action, '\\n','Reward: ', rewards, '\\n','Done: ', dones, '\\n','Next state: ', obs, '\\n','\\n')\n",
    "    curr_state = obs\n",
    "    #env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
